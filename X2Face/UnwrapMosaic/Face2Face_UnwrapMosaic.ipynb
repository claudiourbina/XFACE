{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This jupyter notebook contains demo code for:\n",
    "- loading a model and using it to \n",
    "- drive one or more source frames with a set of driving frames\n",
    "- modifying the embedded face to perform video editing for both the dragon tattoo and Harry Potter scar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (UnwrappedFace.py, line 165)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[1;36m(most recent call last)\u001b[0m:\n",
      "  File \u001b[0;32m\"D:\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\"\u001b[0m, line \u001b[0;32m3267\u001b[0m, in \u001b[0;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-1-6c5b5f759e05>\"\u001b[1;36m, line \u001b[1;32m9\u001b[1;36m, in \u001b[1;35m<module>\u001b[1;36m\u001b[0m\n\u001b[1;33m    from UnwrappedFace import UnwrappedFaceWeightedAverage, UnwrappedFaceWeightedAveragePose\u001b[0m\n",
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\ClaudioVC\\Documents\\Python Scripts\\Universidad\\Procesamiento Avanzado de Im치genes\\Proyecto\\X2Face-master\\UnwrapMosaic\\UnwrappedFace.py\"\u001b[1;36m, line \u001b[1;32m165\u001b[0m\n\u001b[1;33m    sampler_xy = sampler_xy.permute(0,2,3,1) + stddev.permute(0,2,3,1).contiguous().mul(Variable(torch.randn(xs.size())requires_grad=False)) + Variable(xs, requires_grad=False) # choose values according to the std dev\u001b[0m\n\u001b[1;37m                                                                                                                                   ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torch.autograd import Variable\n",
    "from UnwrappedFace import UnwrappedFaceWeightedAverage, UnwrappedFaceWeightedAveragePose\n",
    "import torchvision\n",
    "from torchvision.transforms import ToTensor, Compose, Scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_batch(source_images, pose_images, requires_grad=False, volatile=False):\n",
    "    return model(pose_images, *source_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-a9435c07e228>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;31m#model = torch.load('release_models/x2face_model.pth')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mUnwrappedFaceWeightedAverage\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_num_channels\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_num_channels\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minner_nc\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m128\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'state_dict'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\Python Scripts\\Universidad\\Procesamiento Avanzado de Im치genes\\Proyecto\\X2Face-master\\UnwrapMosaic\\UnwrappedFace.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, output_num_channels, input_num_channels, inner_nc)\u001b[0m\n\u001b[0;32m     17\u001b[0m                 \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mUnwrappedFaceWeightedAverage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpix2pixUnwrapped\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPix2PixModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpix2pixSampler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mNoSkipPix2PixModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_num_channels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_num_channels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minner_nc\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minner_nc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\Python Scripts\\Universidad\\Procesamiento Avanzado de Im치genes\\Proyecto\\X2Face-master\\UnwrapMosaic\\SkipNet.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, output_nc, input_nc)\u001b[0m\n\u001b[0;32m    180\u001b[0m                 \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mPix2PixModel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    181\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 182\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnetG\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdefine_G\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_nc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_nc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m64\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'unet_256'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'batch'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'xavier'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    183\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    184\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mcycles\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\Python Scripts\\Universidad\\Procesamiento Avanzado de Im치genes\\Proyecto\\X2Face-master\\UnwrapMosaic\\SkipNet.py\u001b[0m in \u001b[0;36mdefine_G\u001b[1;34m(input_nc, output_nc, ngf, which_model_netG, norm, use_dropout, init_type, gpu_ids)\u001b[0m\n\u001b[0;32m    149\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    150\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0muse_gpu\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 151\u001b[1;33m         \u001b[1;32massert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    152\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    153\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mwhich_model_netG\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'resnet_9blocks'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#BASE_MODEL = 'release_models/' # Change to your path\n",
    "from functools import partial\n",
    "import pickle\n",
    "pickle.load = partial(pickle.load, encoding=\"latin1\")\n",
    "pickle.Unpickler = partial(pickle.Unpickler, encoding=\"latin1\")\n",
    "\n",
    "model = torch.load('release_models/x2face_model.pth', map_location=lambda storage, loc: storage, pickle_module=pickle)\n",
    "#model = torch.load('release_models/x2face_model.pth')\n",
    "\n",
    "model = UnwrappedFaceWeightedAverage(output_num_channels=2, input_num_channels=3, inner_nc=128)\n",
    "model.load_state_dict(state_dict['state_dict'])\n",
    "model = model.cuda()\n",
    "\n",
    "model = model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Code for driving with another set of frames**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver_path = './examples/Taylor_Swift/1.6/nuBaabkzzzI/'\n",
    "source_path = './examples/Taylor_Swift/1.6/vBgiDYBCuxY/'\n",
    "\n",
    "driver_imgs = [driver_path + d for d in sorted(os.listdir(driver_path))][0:8] # 8 driving frames\n",
    "source_imgs  = [source_path + d for d in sorted(os.listdir(source_path))][0:3] # 3 source frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_img(file_path):\n",
    "    img = Image.open(file_path)\n",
    "    transform = Compose([Scale((256,256)), ToTensor()])\n",
    "    return Variable(transform(img)).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Driving the source image with the driving sequence\n",
    "source_images = []\n",
    "for img in source_imgs:\n",
    "    source_images.append(load_img(img).unsqueeze(0).repeat(len(driver_imgs), 1, 1, 1))\n",
    "    \n",
    "driver_images = None\n",
    "for img in driver_imgs:\n",
    "    if driver_images is None:\n",
    "        driver_images = load_img(img).unsqueeze(0)\n",
    "    else:\n",
    "        driver_images = torch.cat((driver_images, load_img(img).unsqueeze(0)), 0)\n",
    "\n",
    "# Run the model for each\n",
    "result = run_batch(source_images, driver_images)\n",
    "result = result.clamp(min=0, max=1)\n",
    "img = torchvision.utils.make_grid(result.cpu().data)\n",
    "    \n",
    "# Visualise the results\n",
    "fig_size = plt.rcParams[\"figure.figsize\"]\n",
    "fig_size[0] = 24.\n",
    "fig_size[1] = 24.\n",
    "plt.rcParams[\"figure.figsize\"] = fig_size\n",
    "plt.axis('off')\n",
    "\n",
    "result_images = img.permute(1,2,0).numpy()\n",
    "driving_images = torchvision.utils.make_grid(driver_images.cpu().data).permute(1,2,0).numpy()\n",
    "print(\"The results is: \")\n",
    "plt.imshow(np.vstack((result_images, driving_images)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Code for using the dragon tattoo to modify the unwrapped mosaic**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the dragon tattoo\n",
    "fig_size = plt.rcParams[\"figure.figsize\"]\n",
    "fig_size[0] = 1.\n",
    "fig_size[1] = 1.\n",
    "plt.rcParams[\"figure.figsize\"] = fig_size\n",
    "\n",
    "tattoo = Image.open('./tattoos/dragon_tattoo.png')\n",
    "tattoo = tattoo.resize((30,30), Image.BILINEAR).rotate(45).convert('RGBA')\n",
    "\n",
    "b, g, r, a = tattoo.split()\n",
    "tattoo = Image.merge(\"RGBA\", (g, b, r, a))\n",
    "tattoo_flipped = tattoo.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "\n",
    "tattoo = np.array(tattoo) / 256.\n",
    "tattoo[:,:,[1,2]] = 1\n",
    "tattoo_flipped = np.array(tattoo_flipped) / 256.\n",
    "tattoo_flipped[:,:,[1,2]] = 1\n",
    "print(\"The tattoo: \")\n",
    "plt.imshow(tattoo)\n",
    "\n",
    "# Add a flipped version to make a moustache\n",
    "tattoo = torch.Tensor(tattoo)\n",
    "tattoo_flipped = torch.Tensor(tattoo_flipped)\n",
    "tattoo = tattoo.permute(2,0,1)\n",
    "tattoo_flipped = tattoo_flipped.permute(2,0,1)\n",
    "\n",
    "# Changes the colour\n",
    "alpha = tattoo[3:4,:,:].cuda()\n",
    "tattoo = tattoo[0:3,:,:].cuda()\n",
    "alpha_flipped = tattoo_flipped[3:4,:,:].cuda()\n",
    "tattoo_flipped = tattoo_flipped[0:3,:,:].cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Reload driving / source frames\n",
    "driver_path = './examples/Taylor_Swift/1.6/nuBaabkzzzI/'\n",
    "source_path = './examples/Taylor_Swift/1.6/nuBaabkzzzI/'\n",
    "\n",
    "driver_imgs = [driver_path + d for d in sorted(os.listdir(driver_path))][0:8] # 8 driving frames\n",
    "source_imgs  = [source_path + d for d in sorted(os.listdir(source_path))][0:1] # 1 source frames\n",
    "\n",
    "source_images = []\n",
    "for img in source_imgs:\n",
    "    source_images.append(load_img(img).unsqueeze(0))\n",
    "    \n",
    "driver_images = None\n",
    "for img in driver_imgs:\n",
    "    if driver_images is None:\n",
    "        driver_images = load_img(img).unsqueeze(0)\n",
    "    else:\n",
    "        driver_images = torch.cat((driver_images, load_img(img).unsqueeze(0)), 0)\n",
    "\n",
    "        \n",
    "# Modify the face with the given tattoo \n",
    "fig_size = plt.rcParams[\"figure.figsize\"]\n",
    "fig_size[0] = 4.\n",
    "fig_size[1] = 4.\n",
    "plt.rcParams[\"figure.figsize\"] = fig_size\n",
    "\n",
    "# Get the unwrapped face\n",
    "sampler = model.pix2pixUnwrapped(*source_images)[0][:,0:2,:,:]\n",
    "xs = np.linspace(-1,1,256)\n",
    "xs = np.meshgrid(xs, xs)  \n",
    "xs = np.stack(xs, 2) \n",
    "xs = torch.Tensor(xs).unsqueeze(0).repeat(1, 1,1,1).cuda()\n",
    "sampler = nn.Tanh()(sampler).permute(0,2,3,1) + Variable(xs, requires_grad=False)\n",
    "\n",
    "# Visualise unwrapped face with tattoo\n",
    "result = nn.functional.grid_sample(source_images[0],  sampler).squeeze().cpu()\n",
    "alpha = alpha.cpu()\n",
    "tattoo = tattoo.cpu()\n",
    "to_copy = tattoo * alpha.float() + (1 - alpha.float()) * result[:,125:155,90:120].data.cpu()\n",
    "to_copy = Variable(to_copy).cuda()\n",
    "result[:,125:155,90:120] = to_copy\n",
    "\n",
    "alpha_flipped = alpha_flipped.cpu()\n",
    "tattoo_flipped = tattoo_flipped.cpu()\n",
    "to_copy = tattoo_flipped * alpha_flipped.float() + (1 - alpha_flipped.float()) * result[:,125:155,150:180].data.cpu()\n",
    "to_copy = Variable(to_copy).cuda()\n",
    "result[:,125:155,150:180] = to_copy\n",
    "plt.imshow(result.data.squeeze().cpu().permute(1,2,0).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the model for each\n",
    "output_sampler = model.pix2pixSampler(driver_images)[0]\n",
    "output_sampler = nn.Tanh()(output_sampler).permute(0,2,3,1) + Variable(xs, requires_grad=False)\n",
    "result = nn.functional.grid_sample(result.unsqueeze(0).repeat(output_sampler.size(0), 1, 1, 1), \n",
    "                                       output_sampler.cpu()).squeeze()\n",
    "    \n",
    "result = result.clamp(min=0, max=1)\n",
    "img = torchvision.utils.make_grid(result.cpu().data)\n",
    "    \n",
    "# Visualise the results\n",
    "fig_size = plt.rcParams[\"figure.figsize\"]\n",
    "fig_size[0] = 24.\n",
    "fig_size[1] = 24.\n",
    "plt.rcParams[\"figure.figsize\"] = fig_size\n",
    "plt.axis('off')\n",
    "\n",
    "result_images = img.permute(1,2,0).numpy()\n",
    "driving_images = torchvision.utils.make_grid(driver_images.cpu().data).permute(1,2,0).numpy()\n",
    "print(\"The unwrapped mosaic driven with the given face is: \")\n",
    "plt.imshow(np.vstack((result_images, driving_images)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Code for using the Harry Potter scar to modify the unwrapped mosaic**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generat the tattoo for the hp scar\n",
    "index = 7\n",
    "fig_size = plt.rcParams[\"figure.figsize\"]\n",
    "fig_size[0] = 1.\n",
    "fig_size[1] = 1.\n",
    "plt.rcParams[\"figure.figsize\"] = fig_size\n",
    "from PIL import Image\n",
    "tattoo = Image.open('./tattoos/hp_scar_2.png')\n",
    "tattoo = tattoo.resize((30,30), Image.BILINEAR).rotate(45).convert('RGBA')\n",
    "b, g, r, a = tattoo.split()\n",
    "tattoo = Image.merge(\"RGBA\", (g, b, r, a))\n",
    "\n",
    "tattoo = np.array(tattoo) / 256.\n",
    "tattoo[:,:,[0,1]] = 1\n",
    "tattoo = torch.Tensor(tattoo)\n",
    "tattoo = tattoo.permute(2,0,1)\n",
    "alpha = tattoo[2,:,:] < 0.9\n",
    "tattoo = tattoo[0:3,:,:]\n",
    "\n",
    "# Graph the result concatenated with the original frame across a sequence of frames : does the expression change?\n",
    "# Match the \n",
    "fig_size = plt.rcParams[\"figure.figsize\"]\n",
    "fig_size[0] = 4.\n",
    "fig_size[1] = 4.\n",
    "plt.rcParams[\"figure.figsize\"] = (5,5)\n",
    "\n",
    "# Get the unwrapped face\n",
    "sampler = model.pix2pixUnwrapped(*source_images)[0][:,0:2,:,:]\n",
    "xs = np.linspace(-1,1,256)\n",
    "xs = np.meshgrid(xs, xs)  \n",
    "xs = np.stack(xs, 2) \n",
    "xs = torch.Tensor(xs).unsqueeze(0).repeat(1, 1,1,1).cuda()\n",
    "sampler = nn.Tanh()(sampler).permute(0,2,3,1) + Variable(xs, requires_grad=False)\n",
    "result = nn.functional.grid_sample(source_images[0],  sampler).squeeze()\n",
    "\n",
    "alpha = alpha.float().clamp(max=1.0) # modify to make more/less realistic\n",
    "to_copy = tattoo * alpha.float() + (1 - alpha.float()) * result[:,45:75,120:150].data.cpu()\n",
    "to_copy = Variable(to_copy).cuda()\n",
    "result[:,45:75,120:150] = to_copy\n",
    "plt.imshow(result.data.squeeze().cpu().permute(1,2,0).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the model for each\n",
    "output_sampler = model.pix2pixSampler(driver_images)[0]\n",
    "output_sampler = nn.Tanh()(output_sampler).permute(0,2,3,1) + Variable(xs, requires_grad=False)\n",
    "result = nn.functional.grid_sample(result.unsqueeze(0).repeat(output_sampler.size(0), 1, 1, 1).cpu(), \n",
    "                                       output_sampler.cpu()).squeeze()\n",
    "    \n",
    "result = result.clamp(min=0, max=1)\n",
    "img = torchvision.utils.make_grid(result.cpu().data)\n",
    "    \n",
    "# Visualise the results\n",
    "fig_size = plt.rcParams[\"figure.figsize\"]\n",
    "fig_size[0] = 24.\n",
    "fig_size[1] = 24.\n",
    "plt.rcParams[\"figure.figsize\"] = fig_size\n",
    "plt.axis('off')\n",
    "\n",
    "result_images = img.permute(1,2,0).numpy()\n",
    "driving_images = torchvision.utils.make_grid(driver_images.cpu().data).permute(1,2,0).numpy()\n",
    "print(\"The unwrapped mosaic driven with the given face is: \")\n",
    "plt.imshow(np.vstack((result_images, driving_images)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
